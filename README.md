# Daisy - Workshop Scraper & API

SystÃ¨me de scraping et d'API pour collecter et gÃ©rer les ateliers crÃ©atifs depuis Wecandoo.

## ðŸš€ DÃ©marrage rapide

```bash
# 1. Cloner et installer
git clone <repository-url>
cd daisy-scraping
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
playwright install

# 2. DÃ©marrer les services Docker
docker-compose up -d

# 3. DÃ©marrer l'API (Terminal 1)
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

# 4. DÃ©marrer le worker Celery (Terminal 2)
celery -A api.celery_config worker --loglevel=info

# 5. Lancer un crawl
curl -X POST http://localhost:8000/api/v1/start-crawl/wecandoo
```

## Architecture

Le projet est composÃ© de plusieurs services :

- **API FastAPI** : API REST pour gÃ©rer les ateliers (port 8000)
- **Scrapy Spider** : Spider avec support Playwright pour scraper les ateliers
- **Celery Worker** : Gestion des tÃ¢ches asynchrones de scraping
- **PostgreSQL** : Base de donnÃ©es pour stocker les ateliers (port 5666)
- **Redis** : Message broker pour Celery (port 6381)
- **n8n** : Plateforme d'automatisation (port 5678, optionnel)

## Structure du projet

```
daisy/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ atelier.py           # ModÃ¨les SQLModel
â”‚   â”‚   â””â”€â”€ crawl_log.py         # ModÃ¨les Crawl
â”‚   â”œâ”€â”€ main.py                  # Application FastAPI
â”‚   â”œâ”€â”€ tasks.py                 # TÃ¢ches Celery
â”‚   â””â”€â”€ celery_config.py         # Configuration Celery
â”œâ”€â”€ scrapping/
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â””â”€â”€ wecandoo.py          # Spider Wecandoo
â”‚   â”œâ”€â”€ items.py                 # DÃ©finition des items
â”‚   â”œâ”€â”€ pipelines.py             # Pipelines de traitement
â”‚   â””â”€â”€ settings.py              # Configuration Scrapy
â”œâ”€â”€ docker-compose.yml           # Configuration Docker
â”œâ”€â”€ requirements.txt             # DÃ©pendances Python
â””â”€â”€ scrapy.cfg                   # Configuration Scrapy
```

## PrÃ©requis

- Python 3.10+
- Docker & Docker Compose
- pip

## Installation

### 1. Cloner le projet

```bash
git clone <repository-url>
cd daisy
```

### 2. Installer les dÃ©pendances Python

```bash
python -m venv .venv
source .venv/bin/activate  # Sur Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Installer les navigateurs Playwright

```bash
playwright install
```

### 4. DÃ©marrer les services Docker

```bash
docker-compose up -d
```

Cela va dÃ©marrer :
- PostgreSQL sur le port `5666`
- Redis sur le port `6381`
- n8n sur le port `5678`

## Utilisation

### DÃ©marrer les services (dans l'ordre)

#### 1. Services Docker (PostgreSQL, Redis, n8n)

```bash
docker-compose up -d
```

VÃ©rifier que les services tournent :
```bash
docker ps
```

#### 2. API FastAPI (Terminal 1)

```bash
source .venv/bin/activate  # Activer l'environnement virtuel
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

L'API sera accessible sur http://localhost:8000

Documentation : http://localhost:8000/docs

#### 3. Worker Celery (Terminal 2)

**OBLIGATOIRE pour les crawls asynchrones**

```bash
source .venv/bin/activate  # Activer l'environnement virtuel
celery -A api.celery_config worker --loglevel=info
```

Vous devriez voir :
```
[tasks]
  . api.tasks.run_scrapy_spider

celery@... ready.
```

### Lancer le scraping

#### Option 1 : Via Scrapy directement

```bash
cd scrapping
scrapy crawl wecandoo
```

Options disponibles :
- `max_pages` : Nombre maximum de pages Ã  scraper (dÃ©faut: 10)
- `scroll_attempts` : Nombre de scrolls par page (dÃ©faut: 5)

```bash
scrapy crawl wecandoo -a max_pages=20 -a scroll_attempts=10
```

#### Option 2 : Via l'API (asynchrone avec Celery)

```bash
curl -X POST http://localhost:8000/api/v1/start-crawl/wecandoo
```

## API Endpoints

### GET /api/v1/ateliers

RÃ©cupÃ©rer la liste des ateliers

**Query Parameters:**
- `offset` (int, default=0) : DÃ©calage pour la pagination
- `limit` (int, default=100, max=100) : Nombre d'ateliers Ã  retourner
- `category` (string, optionnel) : Filtrer par catÃ©gorie

**Exemple:**
```bash
curl "http://localhost:8000/api/v1/ateliers?limit=10&offset=0"
curl "http://localhost:8000/api/v1/ateliers?category=Poterie"
```

### GET /api/v1/ateliers/{atelier_id}

RÃ©cupÃ©rer un atelier spÃ©cifique

**Exemple:**
```bash
curl http://localhost:8000/api/v1/ateliers/1
```

### GET /api/v1/ateliers/urls

RÃ©cupÃ©rer la liste de toutes les URLs des ateliers (utile pour la dÃ©duplication)

**Exemple:**
```bash
curl http://localhost:8000/api/v1/ateliers/urls
```

**RÃ©ponse:**
```json
[
  "https://wecandoo.fr/atelier/...",
  "https://wecandoo.fr/atelier/...",
  ...
]
```

### POST /api/v1/ateliers/batch

CrÃ©er plusieurs ateliers en batch

**Body:**
```json
[
  {
    "title": "Atelier Poterie",
    "url": "https://wecandoo.fr/atelier/...",
    "category": "Poterie",
    "price": 75.0,
    "duration": "3h",
    "location": "Paris 11e"
  }
]
```

### POST /api/v1/start-crawl/{spider_name}

DÃ©marrer un crawl asynchrone via Celery

**ParamÃ¨tres:**
- `spider_name` : Nom du spider (actuellement: `wecandoo`)

**Exemple:**
```bash
curl -X POST http://localhost:8000/api/v1/start-crawl/wecandoo
```

**RÃ©ponse:**
```json
{
  "task_id": "abc123...",
  "status": "started",
  "message": "crawl wecandoo dÃ©marrÃ© avec succÃ¨s"
}
```

### GET /api/v1/start-crawl/status/{task_id}

VÃ©rifier le statut d'un crawl en cours

**Exemple:**
```bash
# RÃ©cupÃ©rer le task_id de la rÃ©ponse du POST /start-crawl
curl http://localhost:8000/api/v1/start-crawl/status/abc123...
```

**RÃ©ponse:**
```json
{
  "task_id": "abc123...",
  "celery_state": "SUCCESS",
  "status": "SUCCESS",
  "items_scraped": 680,
  "error_message": null,
  "created_at": "2025-11-13T19:21:10.727735",
  "completed_at": "2025-11-13T19:23:45.123456"
}
```

**Statuts possibles:**
- `PENDING` : TÃ¢che en attente
- `STARTED` : TÃ¢che dÃ©marrÃ©e
- `PROGRESS` : En cours d'exÃ©cution
- `SUCCESS` : TerminÃ© avec succÃ¨s
- `FAILED` : Ã‰chec

### DELETE /api/v1/ateliers-all/

Supprimer tous les ateliers de la base de donnÃ©es

**Exemple:**
```bash
curl -X DELETE http://localhost:8000/api/v1/ateliers-all/
```

## Configuration

### Base de donnÃ©es

Par dÃ©faut, l'API se connecte Ã  PostgreSQL :
- Host: `localhost`
- Port: `5666`
- Database: `db`
- User: `postgres`
- Password: `postgres`

Pour modifier la configuration, Ã©ditez [api/main.py:22](api/main.py#L22)

### Celery & Redis

Configuration dans [api/celery_config.py](api/celery_config.py) :
- Broker: `redis://localhost:6381/0`
- Backend: `redis://localhost:6381/0`

### Scrapy

Configuration dans [scrapping/settings.py](scrapping/settings.py) :
- Respect du `robots.txt` : ActivÃ©
- DÃ©lai entre requÃªtes : 3 secondes (avec randomisation)
- AutoThrottle : ActivÃ© (ajuste automatiquement entre 3 et 10 secondes)
- Timeout : 30 minutes
- Concurrence par domaine : 1
- Resources bloquÃ©es : images, stylesheets, fonts, media

## ModÃ¨le de donnÃ©es

### Atelier

```python
{
  "id": int,
  "title": str,              # Titre de l'atelier
  "url": str,                # URL de l'atelier
  "category": str | None,    # CatÃ©gorie (ex: "Poterie", "Couture")
  "price": float | None,     # Prix en euros
  "duration": str | None,    # DurÃ©e (ex: "3h", "2h30")
  "location": str | None     # Localisation (ex: "Paris 11e")
}
```

## Pipeline de traitement

Le scraping suit ce flux :

1. **Spider Wecandoo** â†’ Extrait les donnÃ©es brutes
2. **AtelierPipeline** â†’ Nettoie et normalise les donnÃ©es
3. **DatabasePipeline** â†’ VÃ©rifie les doublons et envoie par batch Ã  l'API
4. **API** â†’ Stocke dans PostgreSQL

## Documentation interactive

Une fois l'API dÃ©marrÃ©e, accÃ©dez Ã  la documentation Swagger :
- http://localhost:8000/docs (Swagger UI)
- http://localhost:8000/redoc (ReDoc)

## AccÃ¨s aux services

- **API** : http://localhost:8000
- **PostgreSQL** : `localhost:5666`
- **Redis** : `localhost:6381`
- **n8n** : http://localhost:5678

## Connexion Ã  PostgreSQL

```bash
psql -h localhost -p 5666 -U postgres -d db
```

Password: `postgres`

## ðŸ”§ Troubleshooting

### Le crawl reste en "PENDING" indÃ©finiment

**Cause:** Le worker Celery n'est pas dÃ©marrÃ©.

**Solution:**
```bash
# VÃ©rifier si le worker tourne
ps aux | grep "celery.*worker" | grep -v grep

# S'il ne tourne pas, le dÃ©marrer
source .venv/bin/activate
celery -A api.celery_config worker --loglevel=info
```

### 429 Rate Limit / Timeout sur le scraping

**Cause:** Trop de requÃªtes trop rapidement vers le site cible.

**Solution:** Les dÃ©lais sont configurÃ©s dans [scrapping/settings.py](scrapping/settings.py):
- `DOWNLOAD_DELAY = 3`
- `RANDOMIZE_DOWNLOAD_DELAY = True`
- `AUTOTHROTTLE_ENABLED = True`

### Duplicate Nodename Warning (Celery)

**Cause:** Plusieurs workers Celery tournent avec le mÃªme nom.

**Solution:**
```bash
# Tuer tous les workers
pkill -f "celery.*worker"

# Relancer un seul worker
celery -A api.celery_config worker --loglevel=info
```

## Notes

- Le worker Celery doit Ãªtre **toujours actif** pour traiter les crawls asynchrones
- Les crawls peuvent prendre 1-2 minutes selon le nombre de pages
- Le dÃ©lai entre requÃªtes Ã©vite le rate limiting (429 errors)
- Les donnÃ©es sont automatiquement dÃ©dupliquÃ©es par URL
